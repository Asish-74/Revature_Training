# Databricks notebook source
# MAGIC %md
# MAGIC ## Installing Library & Reading Excel

# COMMAND ----------

# MAGIC %md
# MAGIC # **Bronze Layer**

# COMMAND ----------

# MAGIC %pip install openpyxl

# COMMAND ----------

# MAGIC %md
# MAGIC ## Converting Pandas â†’ Spark DataFrame

# COMMAND ----------

import pandas as pd
pdf= pd.read_excel("/Volumes/workspace/default/myvolume/Financial Sample.xlsx")

df=spark.createDataFrame(pdf)
display(df)
df.printSchema()

# COMMAND ----------

# MAGIC %md
# MAGIC # **SILVER LAYER**

# COMMAND ----------

for col_name in df.columns:
    df=df.withColumnRenamed(col_name, col_name.replace(" ", "_"))

# COMMAND ----------

from pyspark.sql.functions import col

num_cols= [
    "Units_Sold",
    "Manufacturing_Price",
    "Sale_Price",
    "Gross_Sales",
    "Discounts",
    "_Sales",
    "COGS",
    "_Profit"
]

for c in num_cols:
    if c in df.columns:
        df=df.withColumn(c, col(c).cast("double"))

# COMMAND ----------

df_silver= df.dropna()
display(df_silver)

# COMMAND ----------

# MAGIC %md
# MAGIC # Gold Layer

# COMMAND ----------

from pyspark.sql import functions as F
df_gold= df_silver.groupBy("Country").agg(
    F.sum("_Sales").alias("Total_Sales"),
    F.sum("Profit").alias("Total_Profit"),
    F.sum("COGS").alias("Total_COGS"),
    F.avg("Discounts").alias("Avg_Discounts")
    )
display(df_gold)


# COMMAND ----------

# Total Sales By Country

from pyspark.sql.functions import sum

df_country_sales={
    df_silver.groupBy("Country")
    .agg(sum("_Sales").alias("Total_Sales"))
    .orderBy("Total_Sales", ascending=False)
}

display(df_country_sales)

# COMMAND ----------

# MAGIC %md
# MAGIC **## âœ… BLOCK 2 â€” Converting Pandas â†’ Spark DataFrame**
# MAGIC Code
# MAGIC df = spark.createDataFrame(pdf)
# MAGIC display(df)
# MAGIC df.printSchema()
# MAGIC
# MAGIC ### Explanation
# MAGIC 4ï¸âƒ£ spark.createDataFrame(pdf)
# MAGIC spark â†’ SparkSession object
# MAGIC createDataFrame() â†’ converts Pandas DataFrame â†’ Spark DataFrame
# MAGIC pdf â†’ your pandas data
# MAGIC df â†’ Spark DataFrame created from the Excel data
# MAGIC ### ğŸ‘‰ Why need to convert?
# MAGIC Because Spark transformations require a Spark DataFrame, not Pandas.
# MAGIC
# MAGIC 5ï¸âƒ£ display(df)
# MAGIC Databricks function to display Spark DataFrame in table format.
# MAGIC 6ï¸âƒ£ df.printSchema()
# MAGIC Shows the schema (column names + data types).
# MAGIC
# MAGIC **# âœ… BLOCK 3 â€” Renaming Columns (Remove Spaces)**
# MAGIC Code
# MAGIC for col_name in df.columns:
# MAGIC     df = df.withColumnRenamed(col_name, col_name.replace(" ", "_"))
# MAGIC
# MAGIC Explanation
# MAGIC 7ï¸âƒ£ for col_name in df.columns:
# MAGIC Loops through all column names in DataFrame
# MAGIC df.columns â†’ list of column names
# MAGIC 8ï¸âƒ£ df.withColumnRenamed(old, new)
# MAGIC Renames a column in Spark
# MAGIC old = original name
# MAGIC new = updated name
# MAGIC 9ï¸âƒ£ col_name.replace(" ", "_")
# MAGIC Replaces spaces in column names with _.
# MAGIC Example:
# MAGIC Units Sold â†’ Units_Sold
# MAGIC ğŸ‘‰ Why?
# MAGIC Spaces cause errors in Spark SQL. Underscore is safer.
# MAGIC
# MAGIC
# MAGIC **# âœ… BLOCK 4 â€” Casting Numeric Columns**
# MAGIC 12ï¸âƒ£ df = df.withColumn(c, col(c).cast("double"))
# MAGIC col(c) â†’ selects the column
# MAGIC .cast("double") â†’ change datatype to double
# MAGIC withColumn â†’ update column
# MAGIC ğŸ‘‰ Why casting needed?
# MAGIC Excel sometimes loads numeric values as string, and Spark cannot perform sum/avg on string values.
# MAGIC
# MAGIC ## **BLOCK 6 â€” Creating Gold Layer (Aggregations)**
# MAGIC Code
# MAGIC from pyspark.sql import functions as F
# MAGIC df_gold = df_silver.groupBy("Country").agg(
# MAGIC     F.sum("_Sales").alias("Total_Sales"),
# MAGIC     F.sum("Profit").alias("Total_Profit"),
# MAGIC     F.sum("COGS").alias("Total_COGS"),
# MAGIC     F.avg("Discounts").alias("Avg_Discounts")
# MAGIC )
# MAGIC display(df_gold)
# MAGIC 14ï¸âƒ£ groupBy("Country")
# MAGIC Group data by Country â†’ creates groups.
# MAGIC 15ï¸âƒ£ agg()
# MAGIC Apply multiple aggregations.
# MAGIC 16ï¸âƒ£ F.sum("_Sales")
# MAGIC Sum all Sales per country.
# MAGIC 17ï¸âƒ£ .alias("Total_Sales")
# MAGIC Rename column.
# MAGIC 18ï¸âƒ£ df_gold
# MAGIC Your final Gold layer containing aggregated metrics.
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC ##  **FULL WORKFLOW OF DATABRICKS**
# MAGIC
# MAGIC # â­ WHEN YOU HAVE 2 OR MORE DATASETS â€” WHAT IS THE FIRST STEP?
# MAGIC
# MAGIC The first step is always:
# MAGIC
# MAGIC ### âœ… **Step 1: Bring all datasets into the BRONZE layer (Raw Layer)**
# MAGIC
# MAGIC No cleaning.
# MAGIC No changing.
# MAGIC No filtering.
# MAGIC No joining.
# MAGIC Just **store the raw files exactly as they are**.
# MAGIC
# MAGIC ### âœ” What you should do:
# MAGIC
# MAGIC * Read Dataset 1
# MAGIC * Read Dataset 2
# MAGIC * Read Dataset 3
# MAGIC * â€¦
# MAGIC * Save all of them as **Bronze Delta Tables**
# MAGIC
# MAGIC ### âœ” Why?
# MAGIC
# MAGIC Because raw data must be preserved for audit, debugging, and reprocessing.
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## â­ FULL PROCESS (VERY IMPORTANT)
# MAGIC
# MAGIC Below is your full professional pipeline:
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## ğŸŸ« **STEP 1: BRONZE LAYER (Raw Ingestion)**
# MAGIC
# MAGIC ### WHAT YOU DO:
# MAGIC
# MAGIC * Load Excel, CSV, JSON, Parquet, or API data
# MAGIC * No cleaning
# MAGIC * Convert to Spark DataFrame
# MAGIC * Save as raw Delta Table
# MAGIC
# MAGIC ### EXAMPLE:
# MAGIC
# MAGIC ```python
# MAGIC df1_raw = spark.read.format("csv").option("header", "true").load("path/file1.csv")
# MAGIC df2_raw = spark.read.format("csv").option("header", "true").load("path/file2.csv")
# MAGIC
# MAGIC df1_raw.write.format("delta").saveAsTable("bronze.file1")
# MAGIC df2_raw.write.format("delta").saveAsTable("bronze.file2")
# MAGIC ```
# MAGIC
# MAGIC ### PURPOSE:
# MAGIC
# MAGIC * Keep original data safe
# MAGIC * No changes to schema
# MAGIC * Acts as single-source-of-truth
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC # ğŸŸª **STEP 2: SILVER LAYER (Cleaning + Standardizing)**
# MAGIC
# MAGIC ### WHAT YOU DO:
# MAGIC
# MAGIC For **each dataset separately**, clean and transform:
# MAGIC
# MAGIC ### âœ” Cleaning operations:
# MAGIC
# MAGIC * Rename columns
# MAGIC * Fix datatypes
# MAGIC * Remove null/bad rows
# MAGIC * Remove duplicates
# MAGIC * Standardize formats (dates, names, IDs)
# MAGIC * Apply data quality rules
# MAGIC
# MAGIC ### EXAMPLE:
# MAGIC
# MAGIC ```python
# MAGIC df1_silver = df1_raw \
# MAGIC     .withColumnRenamed("Units Sold", "Units_Sold") \
# MAGIC     .dropna()
# MAGIC
# MAGIC df2_silver = df2_raw \
# MAGIC     .withColumn("Price", col("Price").cast("double"))
# MAGIC ```
# MAGIC
# MAGIC ### PURPOSE:
# MAGIC
# MAGIC * Make data usable
# MAGIC * Prepare for joining
# MAGIC * Ensure schema consistency
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## â­ **WHEN DO I JOIN TWO DATASETS?**
# MAGIC
# MAGIC JOINING ALWAYS HAPPENS IN **SILVER or GOLD**, not Bronze.
# MAGIC
# MAGIC ### If you want:
# MAGIC
# MAGIC * Cleaned + Joined data â†’ **Silver**
# MAGIC * Business metrics â†’ **Gold**
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC # ğŸŸ¨ **STEP 3: GOLD LAYER (Business Aggregations + KPIs)**
# MAGIC
# MAGIC ### WHAT YOU DO:
# MAGIC
# MAGIC * Join Dataset 1 + Dataset 2 using common keys
# MAGIC * Create reports, summaries, dashboards
# MAGIC * Calculate KPIs
# MAGIC
# MAGIC ### EXAMPLE:
# MAGIC
# MAGIC ```python
# MAGIC df_gold = df1_silver.join(df2_silver, "Customer_ID") \
# MAGIC     .groupBy("Country") \
# MAGIC     .agg(sum("Sales").alias("Total_Sales"))
# MAGIC ```
# MAGIC
# MAGIC ### PURPOSE:
# MAGIC
# MAGIC * Ready for BI tools (Power BI, Tableau)
# MAGIC * Business analytics layer
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## â­ **FULL VISUAL PIPELINE**
# MAGIC
# MAGIC ```
# MAGIC           DATASET 1    DATASET 2    DATASET 3
# MAGIC
# MAGIC                 â†“          â†“           â†“
# MAGIC         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# MAGIC         â”‚               ğŸŸ« BRONZE LAYER             â”‚
# MAGIC         â”‚     Raw files (no cleaning, no join)     â”‚
# MAGIC         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# MAGIC                          â†“
# MAGIC         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# MAGIC         â”‚               ğŸŸª SILVER LAYER             â”‚
# MAGIC         â”‚ Cleaning, fixing schema, datatypes,      â”‚
# MAGIC         â”‚ removing nulls, JOINING datasets          â”‚
# MAGIC         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# MAGIC                          â†“
# MAGIC         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
# MAGIC         â”‚               ğŸŸ¨ GOLD LAYER               â”‚
# MAGIC         â”‚ Business reports, KPIs, aggregations     â”‚
# MAGIC         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
# MAGIC ```
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## â­ WHAT YOU MUST ALWAYS DO FIRST WHEN YOU HAVE 2 DATASETS
# MAGIC
# MAGIC ### ğŸ”¥ **Step 1: Bring both datasets into BRONZE exactly as they are**
# MAGIC
# MAGIC âœ” Dataset 1 â†’ Bronze
# MAGIC âœ” Dataset 2 â†’ Bronze
# MAGIC
# MAGIC AFTER THAT:
# MAGIC
# MAGIC ### âœ” Step 2: Clean them individually â†’ Silver
# MAGIC
# MAGIC ### âœ” Step 3: Join if needed â†’ Silver/Gold
# MAGIC
# MAGIC ### âœ” Step 4: Create KPIs â†’ Gold
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC # **Data Lake**
# MAGIC
# MAGIC ## Q: What is a Delta Table?
# MAGIC - A Delta Table is an enhanced Parquet-based storage format that provides ACID transactions and versioning.
# MAGIC - It uses a transaction log (_delta_log) to ensure reliability and consistency.
# MAGIC - Supports time travel, schema enforcement, schema evolution, upsert, merge, and delete operations.
# MAGIC - Enables scalable ETL, analytics, streaming, and real-time pipelines in Lakehouse architecture.
# MAGIC - Delta is the default and most powerful table format in Databricks.
# MAGIC
# MAGIC **Delta Table = parquet + ACID transaction**
# MAGIC
# MAGIC 1. ACID Transactions
# MAGIC Reliable writes & updates even under failures.
# MAGIC 2. Schema Enforcement
# MAGIC Prevents bad data from entering your table.
# MAGIC 3. Schema Evolution
# MAGIC Allows adding new columns automatically.
# MAGIC 4. Time Travel
# MAGIC Query previous versions of data.
# MAGIC 5. Upserts (MERGE)
# MAGIC Support for UPDATE + INSERT in one operation.
# MAGIC 6. Deletes & Updates
# MAGIC Hard to do in Parquet â€” easy in Delta.
# MAGIC 7. Audit & Versioning
# MAGIC Every write becomes a new version.
# MAGIC
# MAGIC ### **What Databricks does:**
# MAGIC df1_raw.write.format("delta").saveAsTable("bronze.file1")
# MAGIC ğŸ”¹ Step 1
# MAGIC Writes DataFrame as Parquet files
# MAGIC ğŸ”¹ Step 2
# MAGIC Creates _delta_log folder
# MAGIC ğŸ”¹ Step 3
# MAGIC Registers table inside Hive Metastore under the name bronze.file1
# MAGIC ğŸ”¹ Step 4
# MAGIC
# MAGIC bronze.file1/
# MAGIC    â”œâ”€â”€ part-000.parquet
# MAGIC    â”œâ”€â”€ part-001.parquet
# MAGIC    â””â”€â”€ _delta_log/
# MAGIC          â”œâ”€â”€ 000000000000.json
# MAGIC          â”œâ”€â”€ 000000000001.json
# MAGIC
# MAGIC
# MAGIC